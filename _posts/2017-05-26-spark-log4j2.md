---
layout: post
title: 在Spark中使用log4j2
---
Spark默认使用的log4j 1.2,它的配置文件就在Spark conf文件下，比如说/etc/spark/conf/log4j.properties.

如果我们想要在spark中使用log4j2，则情况比较复杂了。首先需要事先将log4j2的jar包复制到master节点和全部slave节点上，
将它的路径加入到Spark的classpath中，然后将log4j2.xml文件加入到Spark conf中。

把log42.xml放到resources文件夹中，将log4j2的jar事先复制到各个节点的log4j2文件夹中，那么运行spark程序的command就是：

```shell
spark-submit --files "/home/hadoop/resources/log4j2.xml" 
--driver-java-options=-Dlog4j.configurationFile=resources/log4j2.xml 
--conf "spark.executor.extraJavaOptions=-Dlog4j.configurationFile=resources/log4j2.xml"
--conf "spark.driver.extraClassPath=:/home/hadoop/log4j2/*" 
--conf "spark.executor.extraClassPath=:/home/hadoop/log4j2/*"
```

--files 会将log4j2.xml文件复制到yarn每个executor工作目录下面。注意Spark会为复制的文件创建相应的文件夹，
所以在executor中读取文件的路径是`resources/log4j2.xml`而不是`./log4j2.xml`.